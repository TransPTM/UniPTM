{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc06b68f",
   "metadata": {},
   "source": [
    "## Extract PTM from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f39735bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PTM = \"Phosphoserine_0.7\"\n",
    "targetAA = 'S'\n",
    "\n",
    "file_path = r'C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\raw_data_processed.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "filter_condition = data['Modified residue'].str.contains(PTM, na=False)\n",
    "\n",
    "\n",
    "filtered_data = data[filter_condition]\n",
    "\n",
    "output_path = rf'C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_extracted.csv'\n",
    "filtered_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e18b14",
   "metadata": {},
   "source": [
    "## Extract PTM positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f8182a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     PTM_numbers\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(PTM_numbers) \u001b[38;5;28;01mif\u001b[39;00m PTM_numbers \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mNA\n\u001b[1;32m---> 21\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mModified residue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_positions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#new_file_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_with_positions.csv\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mextract_positions\u001b[1;34m(cell)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_positions\u001b[39m(cell):\n\u001b[1;32m---> 15\u001b[0m     PTM_numbers \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern_PTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     PTM_numbers\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(PTM_numbers) \u001b[38;5;28;01mif\u001b[39;00m PTM_numbers \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mNA\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\re.py:241\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m \n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "#file_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_extracted.csv\"\n",
    "file_path = r\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\non-histone acetylation.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "#pattern_PTM = rf\"MOD_RES (\\d+); /note=\\\"{PTM}.*?\\\"\"\n",
    "pattern_PTM = r\"MOD_RES (\\d+); /note=\\\"N6-acetyllysine\\\"\"\n",
    "\n",
    "\n",
    "def extract_positions(cell):\n",
    "    PTM_numbers = re.findall(pattern_PTM, cell)\n",
    "    PTM_numbers.sort(key=int)\n",
    "    return ', '.join(PTM_numbers) if PTM_numbers else pd.NA\n",
    "\n",
    "\n",
    "\n",
    "df['Position'] = df['Modified residue'].apply(extract_positions)\n",
    "\n",
    "\n",
    "df.dropna(subset=['Position'], inplace=True)\n",
    "\n",
    "#new_file_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_with_positions.csv\"\n",
    "new_file_path = r\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\non-histone acetylation_with_positions.csv\"\n",
    "df.to_csv(new_file_path, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391b977",
   "metadata": {},
   "source": [
    "## Check if all positions are extracted from the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "397f17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个文件中有而第二个文件中没有的Entry：\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file1 = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_extracted.csv\"\n",
    "file2 = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_with_positions.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "\n",
    "entries_file1 = set(df1['Entry'])\n",
    "entries_file2 = set(df2['Entry'])\n",
    "\n",
    "unique_entries = entries_file1 - entries_file2\n",
    "\n",
    "print(\"第一个文件中有而第二个文件中没有的Entry：\")\n",
    "for entry in unique_entries:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df0d8d",
   "metadata": {},
   "source": [
    "## Add labels, masks, possition numbers, and tragart amino acid numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d1b1f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 加载数据\n",
    "file_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_with_positions.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def create_label_and_mask(df):\n",
    "    labels = []\n",
    "    masks = []\n",
    "    pos_nums = []  \n",
    "    targetAA_nums = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        length = row['Length']\n",
    "        sequence = row['Sequence']\n",
    "        \n",
    "        # 创建标签数组，初始全为0\n",
    "        label = np.zeros(length, dtype=int)\n",
    "        \n",
    "        # 如果Position列不为空，则将对应位置设为1\n",
    "        if pd.notnull(row['Position']):\n",
    "            positions = [int(pos) - 1 for pos in str(row['Position']).split(', ')]  # 从1开始的位置转换为0开始\n",
    "            label[positions] = 1\n",
    "        \n",
    "        labels.append(label)\n",
    "        pos_nums.append(np.sum(label))\n",
    "        \n",
    "        # 创建mask数组，Y氨基酸位置为1，其他为0\n",
    "        mask = np.array([1 if amino_acid == targetAA else 0 for amino_acid in sequence], dtype=int)\n",
    "        masks.append(mask)\n",
    "        targetAA_nums.append(np.sum(mask))\n",
    "    \n",
    "    # 将标签和mask转换为逗号分隔的字符串形式，并添加到DataFrame中\n",
    "    df['Label'] = [','.join(map(str, label)) for label in labels]\n",
    "    df['Mask'] = [','.join(map(str, mask)) for mask in masks]\n",
    "    df['Pos_num'] = pos_nums\n",
    "    df['TargetAA_num'] = targetAA_nums\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 调用函数处理DataFrame\n",
    "df = create_label_and_mask(df)\n",
    "\n",
    "# 保存修改后的DataFrame到CSV文件\n",
    "output_file_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_with_labels.csv\"  \n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610cb2a",
   "metadata": {},
   "source": [
    "## Check if labels and mask consistent with positions and targetAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a013658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有行都符合条件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载CSV文件\n",
    "file_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_with_labels.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "\n",
    "def verify_positions_and_count(sequence, positions, target_count):\n",
    "    # 分解位置为整数列表，考虑到可能有多个位置\n",
    "    pos_list = [int(pos) for pos in str(positions).split(',') if pos.strip().isdigit()]\n",
    "    \n",
    "    # 验证指定位置上的氨基酸是否为'Y'\n",
    "    positions_correct = all(sequence[pos - 1].upper() == targetAA for pos in pos_list)\n",
    "    \n",
    "    # 计算序列中'Y'的数量\n",
    "    targetAA_count = sequence.upper().count(targetAA)\n",
    "    \n",
    "    # 验证数量是否与目标数量匹配\n",
    "    count_correct = (targetAA_count == target_count)\n",
    "    \n",
    "    return positions_correct, count_correct, targetAA_count\n",
    "\n",
    "# 应用验证函数到每行，并将结果添加到DataFrame\n",
    "df[['Positions_Correct', 'Count_Correct', 'Actual_targetAA_Count']] = df.apply(\n",
    "    lambda row: verify_positions_and_count(row['Sequence'], row['Position'], row['TargetAA_num']), \n",
    "    axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "# 筛选出不符合条件的行\n",
    "incorrect_rows = df[~(df['Positions_Correct'] & df['Count_Correct'])]\n",
    "\n",
    "# 打印不符合条件的行\n",
    "if not incorrect_rows.empty:\n",
    "    print(\"不符合条件的行：\")\n",
    "    print(incorrect_rows)\n",
    "else:\n",
    "    print(\"所有行都符合条件。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f49ba0",
   "metadata": {},
   "source": [
    "## csv to fasta for CD-HIT preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "db92135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def csv_to_fasta_with_mapping(df, fasta_file_path, mapping_file_path):\n",
    "    with open(fasta_file_path, 'w') as fasta_file, open(mapping_file_path, 'w') as mapping_file:\n",
    "        # 写入映射文件的表头\n",
    "        mapping_file.write(\"Entry,Modified residue,Position,Label,Mask,Pos_num,TargetAA_num\\n\")\n",
    "        for _, row in df.iterrows():\n",
    "            # 构建FASTA描述行，仅包含唯一标识符\n",
    "            description = f\">{row['Entry']}\"\n",
    "            sequence = row['Sequence']\n",
    "            # 写入FASTA文件\n",
    "            fasta_file.write(f\"{description}\\n{sequence}\\n\\n\")\n",
    "            # 写入映射文件\n",
    "            mapping_info = f\"{row['Entry']},{row['Modified residue']},{row['Position']},{row['Label']},{row['Mask']},{row['Pos_num']},{row['TargetAA_num']}\\n\"\n",
    "            mapping_file.write(mapping_info)\n",
    "\n",
    "# 指定输出文件路径\n",
    "fasta_file_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_with_labels.fasta\"  \n",
    "mapping_file_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_mapping.csv\"  \n",
    "\n",
    "# 加载CSV数据\n",
    "df = pd.read_csv(rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_with_labels.csv\"  )\n",
    "\n",
    "# 调用函数，生成FASTA和映射文件\n",
    "csv_to_fasta_with_mapping(df, fasta_file_path, mapping_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a3c16",
   "metadata": {},
   "source": [
    "## fasta to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7f4628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of Pos_num: 14347\n",
      "Total sum of TargetAA_num: 168907\n",
      "Total number of rows (excluding header): 3970\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "\n",
    "# 动态构建原始CSV文件路径\n",
    "original_csv_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_with_labels.csv\"\n",
    "df = pd.read_csv(original_csv_path)\n",
    "\n",
    "# 动态构建去冗余后的FASTA文件路径\n",
    "clustered_fasta_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_with_labels_clustered.fasta\"\n",
    "clustered_ids = [record.id for record in SeqIO.parse(clustered_fasta_path, 'fasta')]\n",
    "\n",
    "# 筛选出原始CSV中存在于去冗余FASTA文件中的记录\n",
    "filtered_df = df[df['Entry'].isin(clustered_ids)]\n",
    "\n",
    "# 计算Pos_num和TargetAA_num列的总和\n",
    "pos_num_sum = filtered_df['Pos_num'].sum()\n",
    "target_aa_num_sum = filtered_df['TargetAA_num'].sum()\n",
    "\n",
    "# 获取总行数（不包括表头）\n",
    "total_rows = filtered_df.shape[0]\n",
    "\n",
    "# 打印计算结果\n",
    "print(f\"Total sum of Pos_num: {pos_num_sum}\")\n",
    "print(f\"Total sum of TargetAA_num: {target_aa_num_sum}\")\n",
    "print(f\"Total number of rows (excluding header): {total_rows}\")\n",
    "\n",
    "# 动态构建保存筛选后记录的CSV文件路径，并保存\n",
    "filtered_csv_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_clustered.csv\"\n",
    "filtered_df.to_csv(filtered_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8995359b",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b87ab17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Statistics:\n",
      "Total sum of Pos_num: 11436\n",
      "Total sum of TargetAA_num: 135563\n",
      "Total number of rows: 3176\n",
      "\n",
      "Test Set Statistics:\n",
      "Total sum of Pos_num: 2911\n",
      "Total sum of TargetAA_num: 33344\n",
      "Total number of rows: 794\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 加载数据集\n",
    "data_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_clustered.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 将Number of Positive Samples大于5的情况合并为一个组，并确保所有分组标签为字符串类型\n",
    "data['Pos_num_grouped'] = data['Pos_num'].apply(lambda x: '>5' if x > 5 else str(x))\n",
    "\n",
    "# 初始化StratifiedShuffleSplit，用于保证每个分组的测试集比例为0.2\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# 使用StratifiedShuffleSplit进行初步划分\n",
    "for train_index, test_index in sss.split(np.zeros(data.shape[0]), data['Pos_num_grouped']):\n",
    "    data['Set'] = 'train'  # 先将所有样本标记为训练集\n",
    "    data.loc[test_index, 'Set'] = 'test'  # 然后将分配到测试集的样本标记为测试集\n",
    "\n",
    "# 检查训练集和测试集的实际比例\n",
    "test_ratio = data['Set'].value_counts(normalize=True)['test']\n",
    "\n",
    "# 如果实际测试集比例偏离0.2，进行调整\n",
    "desired_test_ratio = 0.2\n",
    "if test_ratio > desired_test_ratio:\n",
    "    # 测试集过多，需要从测试集移动到训练集\n",
    "    excess_test_samples = int((test_ratio - desired_test_ratio) * data.shape[0])\n",
    "    test_indices = data[data['Set'] == 'test'].index\n",
    "    move_to_train = np.random.choice(test_indices, size=excess_test_samples, replace=False)\n",
    "    data.loc[move_to_train, 'Set'] = 'train'\n",
    "elif test_ratio < desired_test_ratio:\n",
    "    # 测试集过少，需要从训练集移动到测试集\n",
    "    excess_train_samples = int((desired_test_ratio - test_ratio) * data.shape[0])\n",
    "    train_indices = data[data['Set'] == 'train'].index\n",
    "    move_to_test = np.random.choice(train_indices, size=excess_train_samples, replace=False)\n",
    "    data.loc[move_to_test, 'Set'] = 'test'\n",
    "\n",
    "# 删除临时列\n",
    "data.drop('Pos_num_grouped', axis=1, inplace=True)\n",
    "\n",
    "data = data.sort_values(by='Set', ascending=False)\n",
    "\n",
    "# 保存修改后的数据集\n",
    "output_path = rf\"C:\\Users\\mengl\\OneDrive - City University of Hong Kong - Student\\Desktop\\PTM2_data\\{PTM}_clustered_splited.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "output_path\n",
    "\n",
    "# 分别获取训练集和测试集的子集\n",
    "train_data = data[data['Set'] == 'train']\n",
    "test_data = data[data['Set'] == 'test']\n",
    "\n",
    "# 对训练集进行统计\n",
    "train_pos_num_sum = train_data['Pos_num'].sum()\n",
    "train_target_aa_num_sum = train_data['TargetAA_num'].sum()\n",
    "train_total_rows = train_data.shape[0]\n",
    "\n",
    "# 打印训练集的统计结果\n",
    "print(\"Training Set Statistics:\")\n",
    "print(f\"Total sum of Pos_num: {train_pos_num_sum}\")\n",
    "print(f\"Total sum of TargetAA_num: {train_target_aa_num_sum}\")\n",
    "print(f\"Total number of rows: {train_total_rows}\")\n",
    "\n",
    "# 对测试集进行统计\n",
    "test_pos_num_sum = test_data['Pos_num'].sum()\n",
    "test_target_aa_num_sum = test_data['TargetAA_num'].sum()\n",
    "test_total_rows = test_data.shape[0]\n",
    "\n",
    "# 打印测试集的统计结果\n",
    "print(\"\\nTest Set Statistics:\")\n",
    "print(f\"Total sum of Pos_num: {test_pos_num_sum}\")\n",
    "print(f\"Total sum of TargetAA_num: {test_target_aa_num_sum}\")\n",
    "print(f\"Total number of rows: {test_total_rows}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0cbb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
